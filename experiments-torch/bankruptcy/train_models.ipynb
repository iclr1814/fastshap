{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eastern-guatemala",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "oriental-manhattan",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "nutritional-touch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and split data\n",
    "filename = '../data/bankruptcy.csv'\n",
    "df = pd.read_csv(filename, sep=',', index_col=False)\n",
    "\n",
    "# Drop column (always same value)\n",
    "df.drop(columns=[' Net Income Flag'], inplace=True)\n",
    "\n",
    "# Drop two outlier rows (encoding errors)\n",
    "df.drop(df[df[' Revenue per person'] > 1].index, inplace=True)\n",
    "\n",
    "# Split into X, Y\n",
    "values = df.values\n",
    "X, Y = values[:, 1:], values[:, 0]\n",
    "feature_names = list(df.columns)[1:]\n",
    "\n",
    "# Train/val/test split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, Y, test_size=0.2, random_state=0)\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(\n",
    "    X_train, Y_train, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "planned-harris",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural network pre-processing\n",
    "enc = OneHotEncoder(sparse=False)\n",
    "Y_train_oh = enc.fit_transform(np.expand_dims(Y_train, -1))\n",
    "Y_val_oh = enc.fit_transform(np.expand_dims(Y_val, -1))\n",
    "Y_test_oh = enc.transform(np.expand_dims(Y_test, -1))\n",
    "\n",
    "ss = StandardScaler()\n",
    "ss.fit(X_train)\n",
    "X_train_std = ss.transform(X_train)\n",
    "X_val_std = ss.transform(X_val)\n",
    "X_test_std = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afraid-shopper",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "advisory-potential",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "expanded-roller",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:26:00] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "# Set up data\n",
    "dtrain = xgb.DMatrix(X_train, label=Y_train)\n",
    "dval = xgb.DMatrix(X_val, label=Y_val)\n",
    "\n",
    "# Parameters\n",
    "param = {\n",
    "    'max_depth': 6,\n",
    "    'objective': 'binary:logistic',\n",
    "    'nthread': 4\n",
    "}\n",
    "evallist = [(dtrain, 'train'), (dval, 'val')]\n",
    "num_round = 25\n",
    "\n",
    "# Train\n",
    "model = xgb.train(param, dtrain, num_round, evallist, verbose_eval=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "historical-place",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../models/bankruptcy_model.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedicated-float",
   "metadata": {},
   "source": [
    "# Train surrogate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "visible-english",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from fastshap_torch.utils import MaskLayer1d\n",
    "from fastshap_torch import Surrogate, SoftCrossEntropyLoss\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "accompanied-algorithm",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data\n",
    "num_features = X_train.shape[1]\n",
    "Y_train_surrogate = model.predict(dtrain)\n",
    "Y_train_surrogate = np.vstack([1 - Y_train_surrogate, Y_train_surrogate]).T\n",
    "Y_val_surrogate = model.predict(dval)\n",
    "Y_val_surrogate = np.vstack([1 - Y_val_surrogate, Y_val_surrogate]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "secure-proceeding",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss given no information = 0.1453\n"
     ]
    }
   ],
   "source": [
    "# Get loss upper bound\n",
    "p = Y_train_surrogate.mean(axis=0)\n",
    "soft_ce = - np.mean(np.sum(np.log(p) * Y_train_surrogate, axis=1))\n",
    "print('Loss given no information = {:.4f}'.format(soft_ce))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "vanilla-cursor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up device\n",
    "device = torch.device('cuda', 6)\n",
    "\n",
    "# Create model\n",
    "surrogate = nn.Sequential(\n",
    "    MaskLayer1d(value=0, append=True),\n",
    "    nn.Linear(2 * num_features, 128),\n",
    "    nn.ELU(inplace=True),\n",
    "    nn.Linear(128, 128),\n",
    "    nn.ELU(inplace=True),\n",
    "    nn.Linear(128, 2)).to(device)\n",
    "\n",
    "# Set up surrogate object\n",
    "surr = Surrogate(surrogate, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "numerical-freeze",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best loss = 0.0716\n",
      "Best loss = 0.0716\n",
      "Best loss = 0.0716\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "for batch_size in (32, 512, 8192):\n",
    "    surr.train((X_train_std, Y_train_surrogate),\n",
    "               (X_val_std, Y_val_surrogate),\n",
    "               batch_size=batch_size,\n",
    "               max_epochs=100,\n",
    "               loss_fn=SoftCrossEntropyLoss(),\n",
    "               validation_samples=50,\n",
    "               validation_batch_size=10000,\n",
    "               validation_seed=0,\n",
    "               verbose=False)\n",
    "    \n",
    "    print('Best loss = {:.4f}'.format(min(surr.loss_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "completed-piano",
   "metadata": {},
   "outputs": [],
   "source": [
    "surrogate.cpu()\n",
    "surrogate.eval()\n",
    "torch.save(surrogate, '../models/bankruptcy_surrogate.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transsexual-bedroom",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
