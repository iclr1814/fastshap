{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "indonesian-trinidad",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "alien-translation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "objective-continent",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and split data\n",
    "filename = '../data/bankruptcy.csv'\n",
    "df = pd.read_csv(filename, sep=',', index_col=False)\n",
    "\n",
    "# Drop column (always same value)\n",
    "df.drop(columns=[' Net Income Flag'], inplace=True)\n",
    "\n",
    "# Drop two outlier rows (encoding errors)\n",
    "df.drop(df[df[' Revenue per person'] > 1].index, inplace=True)\n",
    "\n",
    "# Split into X, Y\n",
    "values = df.values\n",
    "X, Y = values[:, 1:], values[:, 0]\n",
    "feature_names = list(df.columns)[1:]\n",
    "\n",
    "# Train/val/test split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, Y, test_size=0.2, random_state=0)\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(\n",
    "    X_train, Y_train, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "narrow-shower",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural network pre-processing\n",
    "enc = OneHotEncoder(sparse=False)\n",
    "Y_train_oh = enc.fit_transform(np.expand_dims(Y_train, -1))\n",
    "Y_val_oh = enc.fit_transform(np.expand_dims(Y_val, -1))\n",
    "Y_test_oh = enc.transform(np.expand_dims(Y_test, -1))\n",
    "\n",
    "ss = StandardScaler()\n",
    "ss.fit(X_train)\n",
    "X_train_std = ss.transform(X_train)\n",
    "X_val_std = ss.transform(X_val)\n",
    "X_test_std = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "requested-diamond",
   "metadata": {},
   "source": [
    "# Set up imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "personal-special",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import fastshap_torch\n",
    "from fastshap_torch import Surrogate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "compound-legislation",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda', 3)\n",
    "surrogate = torch.load('../models/bankruptcy_surrogate.pt').eval().to(device)\n",
    "num_features = X_train.shape[1]\n",
    "surr = Surrogate(surrogate, num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "failing-cassette",
   "metadata": {},
   "source": [
    "# FastSHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "operating-luther",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from fastshap_torch import FastSHAP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dried-duplicate",
   "metadata": {},
   "source": [
    "# Test samples number\n",
    "\n",
    "- I'm trying with no penalty because the ablation experiment showed that no penalty was better, at least for 32 samples with paired sampling. But these results are not so good..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "alert-aluminum",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best val loss = 0.00804849\n",
      "Best val loss = 0.00135096\n",
      "Best val loss = 0.00017779\n",
      "Best val loss = 0.00016724\n",
      "Best val loss = 0.00016727\n",
      "Best val loss = 0.00017060\n",
      "Best val loss = 0.00017634\n"
     ]
    }
   ],
   "source": [
    "for n_samples in (1, 4, 16, 32, 48, 64, 96):\n",
    "    # Set up explainer model\n",
    "    explainer = nn.Sequential(\n",
    "        nn.Linear(num_features, 128),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Linear(128, 128),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Linear(128, 2 * num_features)).to(device)\n",
    "\n",
    "    # Set up FastSHAP wrapper\n",
    "    fastshap = FastSHAP(explainer, surr, normalization='additive', link=nn.Softmax(dim=-1))\n",
    "\n",
    "    # Train\n",
    "    fastshap.train(\n",
    "        X_train_std,\n",
    "        X_val_std[:500],\n",
    "        batch_size=32,\n",
    "        lr=2e-4,\n",
    "        min_lr=1e-4,\n",
    "        num_samples=n_samples,\n",
    "        max_epochs=200,\n",
    "        eff_lambda=0,\n",
    "        paired_sampling=False,\n",
    "        validation_samples=128,\n",
    "        validation_seed=123,\n",
    "        lookback=10,\n",
    "        verbose=False)\n",
    "\n",
    "    # Print performance\n",
    "    print('Best val loss = {:.8f}'.format(min(fastshap.loss_list)))\n",
    "\n",
    "    # Save model\n",
    "    modifier = 'samples={}'.format(n_samples)\n",
    "    explainer.cpu()\n",
    "    torch.save(explainer, '../models/bankruptcy_explainer {} nopenalty.pt'.format(modifier))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "intended-optimum",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best val loss = 0.00141389\n",
      "Best val loss = 0.00017716\n",
      "Best val loss = 0.00016697\n",
      "Best val loss = 0.00016942\n",
      "Best val loss = 0.00017018\n",
      "Best val loss = 0.00016867\n"
     ]
    }
   ],
   "source": [
    "for n_samples in (4, 16, 32, 48, 64, 96):\n",
    "# for n_samples in (32, 64):\n",
    "    # Set up explainer model\n",
    "    explainer = nn.Sequential(\n",
    "        nn.Linear(num_features, 128),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Linear(128, 128),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Linear(128, 2 * num_features)).to(device)\n",
    "\n",
    "    # Set up FastSHAP wrapper\n",
    "    fastshap = FastSHAP(explainer, surr, normalization='additive', link=nn.Softmax(dim=-1))\n",
    "\n",
    "    # Train\n",
    "    fastshap.train(\n",
    "        X_train_std,\n",
    "        X_val_std[:500],\n",
    "        batch_size=32,\n",
    "        lr=1e-3,\n",
    "        min_lr=1e-4,\n",
    "        num_samples=n_samples,\n",
    "        max_epochs=200,\n",
    "        eff_lambda=0,\n",
    "        paired_sampling=True,\n",
    "        validation_samples=128,\n",
    "        validation_seed=123,\n",
    "        lookback=10,\n",
    "        verbose=False)\n",
    "\n",
    "    # Print performance\n",
    "    print('Best val loss = {:.8f}'.format(min(fastshap.loss_list)))\n",
    "\n",
    "    # Save model\n",
    "    modifier = 'paired_samples={}'.format(n_samples)\n",
    "    explainer.cpu()\n",
    "    torch.save(explainer, '../models/bankruptcy_explainer {} nopenalty.pt'.format(modifier))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becoming-chemical",
   "metadata": {},
   "source": [
    "# Test other parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "diagnostic-waste",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best val loss = 0.00016934\n"
     ]
    }
   ],
   "source": [
    "# Set up explainer model\n",
    "explainer = nn.Sequential(\n",
    "    nn.Linear(num_features, 128),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(128, 128),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(128, 2 * num_features)).to(device)\n",
    "\n",
    "# Set up FastSHAP wrapper\n",
    "fastshap = FastSHAP(explainer, surr, normalization='additive', link=nn.Softmax(dim=-1))\n",
    "\n",
    "# Train\n",
    "fastshap.train(\n",
    "    X_train_std,\n",
    "    X_val_std[:500],\n",
    "    batch_size=32,\n",
    "    lr=1e-3,\n",
    "    min_lr=1e-4,\n",
    "    num_samples=32,\n",
    "    max_epochs=200,\n",
    "    eff_lambda=0,\n",
    "    paired_sampling=True,\n",
    "    validation_samples=128,\n",
    "    validation_seed=123,\n",
    "    lookback=10,\n",
    "    verbose=False)\n",
    "\n",
    "# Print performance\n",
    "print('Best val loss = {:.8f}'.format(min(fastshap.loss_list)))\n",
    "\n",
    "# Save model\n",
    "modifier = 'nopenalty'\n",
    "explainer.cpu()\n",
    "torch.save(explainer, '../models/bankruptcy_explainer {}.pt'.format(modifier))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fossil-jurisdiction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best val loss = 0.00027661\n"
     ]
    }
   ],
   "source": [
    "# Set up explainer model\n",
    "explainer = nn.Sequential(\n",
    "    nn.Linear(num_features, 128),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(128, 128),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(128, 2 * num_features)).to(device)\n",
    "\n",
    "# Set up FastSHAP wrapper\n",
    "fastshap = FastSHAP(explainer, surr, normalization=None, link=nn.Softmax(dim=-1))\n",
    "\n",
    "# Train\n",
    "fastshap.train(\n",
    "    X_train_std,\n",
    "    X_val_std[:500],\n",
    "    batch_size=32,\n",
    "    lr=1e-3,\n",
    "    min_lr=1e-4,\n",
    "    num_samples=32,\n",
    "    max_epochs=200,\n",
    "    eff_lambda=0.1,\n",
    "    paired_sampling=True,\n",
    "    validation_samples=128,\n",
    "    validation_seed=123,\n",
    "    lookback=10,\n",
    "    verbose=False)\n",
    "\n",
    "# Print performance\n",
    "print('Best val loss = {:.8f}'.format(min(fastshap.loss_list)))\n",
    "\n",
    "# Save model\n",
    "modifier = 'nonormalization'\n",
    "explainer.cpu()\n",
    "torch.save(explainer, '../models/bankruptcy_explainer {}.pt'.format(modifier))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "union-penalty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best val loss = 0.00019273\n"
     ]
    }
   ],
   "source": [
    "# Set up explainer model\n",
    "explainer = nn.Sequential(\n",
    "    nn.Linear(num_features, 128),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(128, 128),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(128, 2 * num_features)).to(device)\n",
    "\n",
    "# Set up FastSHAP wrapper\n",
    "fastshap = FastSHAP(explainer, surr, normalization=None, link=nn.Softmax(dim=-1))\n",
    "\n",
    "# Train\n",
    "fastshap.train(\n",
    "    X_train_std,\n",
    "    X_val_std[:500],\n",
    "    batch_size=32,\n",
    "    lr=1e-3,\n",
    "    min_lr=1e-4,\n",
    "    num_samples=32,\n",
    "    max_epochs=200,\n",
    "    eff_lambda=0,\n",
    "    paired_sampling=True,\n",
    "    validation_samples=128,\n",
    "    validation_seed=123,\n",
    "    lookback=10,\n",
    "    verbose=False)\n",
    "\n",
    "# Print performance\n",
    "print('Best val loss = {:.8f}'.format(min(fastshap.loss_list)))\n",
    "\n",
    "# Save model\n",
    "modifier = 'nopenalty nonormalization'\n",
    "explainer.cpu()\n",
    "torch.save(explainer, '../models/bankruptcy_explainer {}.pt'.format(modifier))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alone-syria",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
